# Нейросетевые генеративные текстовые модели

## Автоэнкодерная модель для генерации текста

Код автоэнкодера содержится в файле [lstm_ae.py](https://github.com/Koziev/pushkin/blob/master/PyCode/lstm_ae.py).
Реализовано две архитектуры:

1) простой сжимающий автоэнкодер для цепочек слов;
2) вариационный автоэнкодер.

Второй вариант накладывает дополнительое условие на распределение значений переменных
на скрытом слое, чтобы обеспечить неразрывность векторного пространства встраиваний
предложений.

Перед тренировкой автоэнкодера необходимо подготовить датасет с помощью [prepare_vae_dataset.py](https://github.com/Koziev/pushkin/blob/master/PyCode/prepare_vae_dataset.py).
Этот код берет предложения из файла [phrases.txt](https://github.com/Koziev/pushkin/blob/master/data/phrases.txt) и загружает список векторов слов
из текстового файла в формате word2vec (путь к нему нужно указать свой, так как выложить
готовый файл невозможно из-за его размера). В результате работы программы будут созданы файлы
с векторизованными предложениями, которые и загрузит код в lstm_ae.py.

Запуск тренировки автоэнкодера:

```
python lstm_ae.py --train 1
```

Расчет гистограмм распределения значений скрытых переменных:

```
python lstm_ae.py --estimate 1
```

Генерация случайного текста после выполнения первых двух этапов:

```
python lstm_ae.py --generate 1
```

## Generative Adversarial Network для генерации предложений

Исключительно экспериментальный код.

Простейший вариант GAN реализован в [gan_words.py](https://github.com/Koziev/pushkin/blob/master/PyCode/gan_words.py).

В качестве входных данных берет, как и вышеописанные автоэнкодерные модели, файлы датасета,
приготовленные [prepare_vae_dataset.py](https://github.com/Koziev/pushkin/blob/master/PyCode/prepare_vae_dataset.py). 

## Character sequence text entailment: продолжение символьной цепочки

Модель реализована на Python 2.x с использованием фреймворка [Keras](https://keras.io/).

### Обучение модели

Обучение реализовано в [char_predictor.py](https://github.com/Koziev/pushkin/blob/master/PyCode/char_predictor.py).

Для обучения модели необходим текстовый файл (plain text, utf-8). Какую-либо нормализацию
текста выполнять не требуется. Модель сама составит список всех используемых символов.

Путь к текстовому файла прописан в [char_predictor.py](https://github.com/Koziev/pushkin/blob/master/PyCode/char_predictor.py).
Там же стоит вторая важная настройка - количество сэмплов в обучающем датасете. Чем большое
сэмплов, тем лучше модель будет работать, но время обучения будет расти пропорционально.

В ходе обучения программа сохраняет параметры модели на диске.

Для nb_patterns = 2000000 обучение с использованием GPU NVidia GTX 980 длится примерно 5 часов,
с учетом 5 отбрасываемых эпох до срабатывания early stopping.


### Проверка работы модели в консоли

Консольный генератор текста реализован в [char_generator.py](https://github.com/Koziev/pushkin/blob/master/PyCode/char_generator.py).
Он загружает модельные данные из файлов, приготовленных в ходе обучения. далее можно вводить 
с клавиатуры цепочку символов (начало фразы), а модель выведет предполагаемое продолжение.
Примерно так:

```
>: поздравляю с новы
поздравляю с новых настоящий собой мо

>: сяду на пенек, съем пиро
сяду на пенек, съем пиров накопления по нее 

>: цветы нужно поливать кажд
цветы нужно поливать каждое серодоразисте так

>: тише, мыши, кот на кры
тише, мыши, кот на крыше и высоких своей т
```

